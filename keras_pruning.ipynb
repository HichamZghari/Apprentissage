{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Keras Model pruning\n",
    "\n",
    "Le pruning, ou élagage en français, c’est l’idée de réduire la taille d’un réseau de neurones, tout en minimisant la perte de performance.\n",
    "La performance étant définie par :\n",
    "    \n",
    "    – Les métriques classiques en Machine Learning\n",
    "    – Le temps d’inférence\n",
    "    – Le nombre de paramètres du réseau\n",
    "    – Etc..\n",
    "\n",
    "Méthodes pour y parvenir:\n",
    "\n",
    "    (M1) Mettre à 0 certains paramètres: \"Weight Pruning\"\n",
    "    (M2) Mettre à 0 des neurones entiers: \"Unit/Neuron pruning\"\n",
    "\n",
    "<img src=\"photo.png\" alt=\"Pruning example\" width=\"500\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as axes\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import tensorboard\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "import zipfile\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "from numpy import linalg as LA\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utilisé la base de données MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 784)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset='mnist'):\n",
    "\n",
    "    # Préciser les dimentions des images\n",
    "    img_rows, img_cols = 28, 28\n",
    "    \n",
    "    if dataset=='mnist':\n",
    "        # Nombre de classe\n",
    "        num_classes = 10\n",
    "        # Découper les données en données d'apprentissage/testes\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    else:\n",
    "        print('dataset name does not match available options \\n( mnist | keras )')\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows*img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows*img_cols)\n",
    "    input_shape = (img_rows*img_cols*1,)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # Convertir les vecteurs en matrices binaires\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, num_classes, input_shape\n",
    "\n",
    "# On charge les données de la base MNIST. \n",
    "\n",
    "mnist_x_train, mnist_x_test, mnist_y_train, mnist_y_test, num_classes, input_shape = load_dataset(dataset='mnist')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainer un modèle de Keras sans élagage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous alons construire un réseau de neurone à 4 couches denses et complétement connectées avec les tailles suivantes: 1000, 1000, 500 et 200. Une cinquième couche pour les logs de résultants (de taille 10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tf.keras.layers\n",
    "\n",
    "\n",
    "\"\"\"  Construire le modèle\n",
    "    \n",
    "    @args: input_shape: Décrire le format des données saisies\n",
    "           num_classes: Le nombre de classe des labels\n",
    "           sparsity: Le facteur de parcimonie\n",
    "           \n",
    "    @return: TF.Keras modèle avec 4 couches denses.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def build_model_arch(input_shape, num_classes, sparsity=0.0):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(l.Dense(int(1000-(1000*sparsity)), activation='relu',\n",
    "                      input_shape=input_shape)),\n",
    "    model.add(l.Dense(int(1000-(1000*sparsity)), activation='relu'))\n",
    "    model.add(l.Dense(int(500-(500*sparsity)), activation='relu'))\n",
    "    model.add(l.Dense(int(200-(200*sparsity)), activation='relu'))\n",
    "    model.add(l.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "mnist_model_base = build_model_arch(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training logs to /tmp/tmplw380ll6\n"
     ]
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "print('Writing training logs to ' + logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 06:49:25.570639: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0136 - accuracy: 0.9968 - val_loss: 0.1242 - val_accuracy: 0.9830\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0148 - accuracy: 0.9964 - val_loss: 0.0979 - val_accuracy: 0.9837\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0135 - accuracy: 0.9968 - val_loss: 0.1287 - val_accuracy: 0.9820\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.1102 - val_accuracy: 0.9780\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0100 - accuracy: 0.9978 - val_loss: 0.1309 - val_accuracy: 0.9843\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0090 - accuracy: 0.9979 - val_loss: 0.0876 - val_accuracy: 0.9823\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0088 - accuracy: 0.9976 - val_loss: 0.0941 - val_accuracy: 0.9828\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.1183 - val_accuracy: 0.9810\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 0.1280 - val_accuracy: 0.9825\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 0.1155 - val_accuracy: 0.9823\n",
      "Test loss: 0.11554555594921112\n",
      "Test accuracy: 0.9822999835014343\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 1000)              785000    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 200)               100200    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,388,710\n",
      "Trainable params: 2,388,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Entrainer le modèle créé\n",
    "\n",
    "@args: model: Keras modèle\n",
    "      x_train: données d'entrainement\n",
    "      y_train: Labels des données d'entrainement\n",
    "      batch_size: la taille du batch\n",
    "      epochs: Le nombre d'époque d'entrainement\n",
    "      x_test: données de tests\n",
    "      y_test: Labels des données de tests\n",
    "      \n",
    "@return: modèle entrainé + statistique (perte + précision)\n",
    "\"\"\"\n",
    "\n",
    "def make_nosparse_model(model, x_train, y_train, batch_size, \n",
    "                         epochs, x_test, y_test):\n",
    "    \n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch=0)]\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    return model, score\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "mnist_model, mnist_score = make_nosparse_model(mnist_model_base,\n",
    "                                               mnist_x_train,\n",
    "                                               mnist_y_train,\n",
    "                                               batch_size,\n",
    "                                               epochs,\n",
    "                                               mnist_x_test,\n",
    "                                               mnist_y_test)\n",
    "print(mnist_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning des couches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Retourner les matrices creuses avec une sparsity = K \"\"\"\n",
    "\n",
    "def weight_prune_dense_layer(k_weights, b_weights, k_sparsity):\n",
    "\n",
    "    # Copy the kernel weights and get ranked indeces of the abs\n",
    "    kernel_weights = np.copy(k_weights)\n",
    "    ind = np.unravel_index(\n",
    "        np.argsort(\n",
    "            np.abs(kernel_weights),\n",
    "            axis=None),\n",
    "        kernel_weights.shape)\n",
    "        \n",
    "    # Number of indexes to set to 0\n",
    "    cutoff = int(len(ind[0])*k_sparsity)\n",
    "    # The indexes in the 2D kernel weight matrix to set to 0\n",
    "    sparse_cutoff_inds = (ind[0][0:cutoff], ind[1][0:cutoff])\n",
    "    kernel_weights[sparse_cutoff_inds] = 0.\n",
    "        \n",
    "    # Copy the bias weights and get ranked indeces of the abs\n",
    "    bias_weights = np.copy(b_weights)\n",
    "    ind = np.unravel_index(\n",
    "        np.argsort(\n",
    "            np.abs(bias_weights), \n",
    "            axis=None), \n",
    "        bias_weights.shape)\n",
    "        \n",
    "    # Number of indexes to set to 0\n",
    "    cutoff = int(len(ind[0])*k_sparsity)\n",
    "    # The indexes in the 1D bias weight matrix to set to 0\n",
    "    sparse_cutoff_inds = (ind[0][0:cutoff])\n",
    "    bias_weights[sparse_cutoff_inds] = 0.\n",
    "    \n",
    "    return kernel_weights, bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Takes in matrices of kernel and bias weights (for a dense\n",
    "      layer) and returns the unit-pruned versions of each\n",
    "@args:\n",
    "      k_weights: 2D matrix \n",
    "      b_weights: 1D matrix of the biases of a dense layer\n",
    "      k_sparsity: percentage of weights to set to 0\n",
    "@return:\n",
    "      kernel_weights: sparse matrix with same shape as the original\n",
    "        kernel weight matrix\n",
    "      bias_weights: sparse array with same shape as the original\n",
    "        bias array\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def unit_prune_dense_layer(k_weights, b_weights, k_sparsity):\n",
    "\n",
    "    # Copy the kernel weights and get ranked indeces of the\n",
    "    # column-wise L2 Norms\n",
    "    kernel_weights = np.copy(k_weights)\n",
    "    ind = np.argsort(LA.norm(kernel_weights, axis=0))\n",
    "        \n",
    "    # Number of indexes to set to 0\n",
    "    cutoff = int(len(ind)*k_sparsity)\n",
    "    # The indexes in the 2D kernel weight matrix to set to 0\n",
    "    sparse_cutoff_inds = ind[0:cutoff]\n",
    "    kernel_weights[:,sparse_cutoff_inds] = 0.\n",
    "        \n",
    "    # Copy the bias weights and get ranked indeces of the abs\n",
    "    bias_weights = np.copy(b_weights)\n",
    "    # The indexes in the 1D bias weight matrix to set to 0\n",
    "    # Equal to the indexes of the columns that were removed in this case\n",
    "    #sparse_cutoff_inds\n",
    "    bias_weights[sparse_cutoff_inds] = 0.\n",
    "    \n",
    "    return kernel_weights, bias_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning sur tout un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_model(model, x_test, y_test, k_sparsity, pruning='weight'):\n",
    "    \"\"\"\n",
    "    Takes in a model made of dense layers and prunes the weights\n",
    "    Args:\n",
    "      model: Keras model\n",
    "      k_sparsity: target sparsity of the model\n",
    "    Returns:\n",
    "      sparse_model: sparsified copy of the previous model\n",
    "    \"\"\"\n",
    "    # Copying a temporary sparse model from our original\n",
    "    sparse_model = tf.keras.models.clone_model(model)\n",
    "    sparse_model.set_weights(model.get_weights())\n",
    "    \n",
    "    # Getting a list of the names of each component (w + b) of each layer\n",
    "    names = [weight.name for layer in sparse_model.layers for weight in layer.weights]\n",
    "    # Getting the list of the weights for each component (w + b) of each layer\n",
    "    weights = sparse_model.get_weights()\n",
    "    \n",
    "    # Initializing list that will contain the new sparse weights\n",
    "    newWeightList = []\n",
    "\n",
    "    # Iterate over all but the final 2 layers (the softmax)\n",
    "    for i in range(0, len(weights)-2, 2):\n",
    "        \n",
    "        if pruning=='weight':\n",
    "            kernel_weights, bias_weights = weight_prune_dense_layer(weights[i],\n",
    "                                                                    weights[i+1],\n",
    "                                                                    k_sparsity)\n",
    "        elif pruning=='unit':\n",
    "            kernel_weights, bias_weights = unit_prune_dense_layer(weights[i],\n",
    "                                                                  weights[i+1],\n",
    "                                                                  k_sparsity)\n",
    "        else:\n",
    "            print('does not match available pruning methods ( weight | unit )')\n",
    "        \n",
    "        # Append the new weight list with our sparsified kernel weights\n",
    "        newWeightList.append(kernel_weights)\n",
    "        \n",
    "        # Append the new weight list with our sparsified bias weights\n",
    "        newWeightList.append(bias_weights)\n",
    "\n",
    "    # Adding the unchanged weights of the final 2 layers\n",
    "    for i in range(len(weights)-2, len(weights)):\n",
    "        unmodified_weight = np.copy(weights[i])\n",
    "        newWeightList.append(unmodified_weight)\n",
    "\n",
    "    # Setting the weights of our model to the new ones\n",
    "    sparse_model.set_weights(newWeightList)\n",
    "    \n",
    "    # Re-compiling the Keras model (necessary for using `evaluate()`)\n",
    "    sparse_model.compile(\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    # Printing the the associated loss & Accuracy for the k% sparsity\n",
    "    score = sparse_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('k% weight sparsity: ', k_sparsity,\n",
    "          '\\tTest loss: {:07.5f}'.format(score[0]),\n",
    "          '\\tTest accuracy: {:05.2f} %%'.format(score[1]*100.))\n",
    "    \n",
    "    return sparse_model, score\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tester les méthodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MNIST Weight-pruning\n",
      "\n",
      "k% weight sparsity:  0.0 \tTest loss: 0.12456 \tTest accuracy: 97.83 %%\n",
      "k% weight sparsity:  0.25 \tTest loss: 0.12211 \tTest accuracy: 97.86 %%\n",
      "k% weight sparsity:  0.5 \tTest loss: 0.11283 \tTest accuracy: 97.78 %%\n",
      "k% weight sparsity:  0.6 \tTest loss: 0.10753 \tTest accuracy: 97.59 %%\n",
      "k% weight sparsity:  0.7 \tTest loss: 0.11549 \tTest accuracy: 97.02 %%\n",
      "k% weight sparsity:  0.8 \tTest loss: 0.21914 \tTest accuracy: 95.64 %%\n",
      "k% weight sparsity:  0.9 \tTest loss: 1.19742 \tTest accuracy: 76.03 %%\n",
      "k% weight sparsity:  0.95 \tTest loss: 2.04617 \tTest accuracy: 19.92 %%\n",
      "k% weight sparsity:  0.97 \tTest loss: 2.24532 \tTest accuracy: 10.80 %%\n",
      "k% weight sparsity:  0.99 \tTest loss: 2.31206 \tTest accuracy: 09.74 %%\n",
      "\n",
      " MNIST Unit-pruning\n",
      "\n",
      "k% weight sparsity:  0.0 \tTest loss: 0.12456 \tTest accuracy: 97.83 %%\n",
      "k% weight sparsity:  0.25 \tTest loss: 0.10615 \tTest accuracy: 97.82 %%\n",
      "k% weight sparsity:  0.5 \tTest loss: 0.09645 \tTest accuracy: 97.98 %%\n",
      "k% weight sparsity:  0.6 \tTest loss: 0.19244 \tTest accuracy: 97.74 %%\n",
      "k% weight sparsity:  0.7 \tTest loss: 0.53066 \tTest accuracy: 96.71 %%\n",
      "k% weight sparsity:  0.8 \tTest loss: 1.48950 \tTest accuracy: 84.38 %%\n",
      "k% weight sparsity:  0.9 \tTest loss: 2.21478 \tTest accuracy: 20.05 %%\n",
      "k% weight sparsity:  0.95 \tTest loss: 2.29939 \tTest accuracy: 09.74 %%\n",
      "k% weight sparsity:  0.97 \tTest loss: 2.30899 \tTest accuracy: 09.74 %%\n",
      "k% weight sparsity:  0.99 \tTest loss: 2.30900 \tTest accuracy: 09.74 %%\n"
     ]
    }
   ],
   "source": [
    "# list of sparsities\n",
    "k_sparsities = [0.0, 0.25, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.97, 0.99]\n",
    "\n",
    "# The empty lists where we will store our training results\n",
    "mnist_model_loss_weight = []\n",
    "mnist_model_accs_weight = []\n",
    "mnist_model_loss_unit = []\n",
    "mnist_model_accs_unit = []\n",
    "fmnist_model_loss_weight = []\n",
    "fmnist_model_accs_weight = []\n",
    "fmnist_model_loss_unit = []\n",
    "fmnist_model_accs_unit = []\n",
    "\n",
    "dataset = 'mnist'\n",
    "pruning = 'weight'\n",
    "print('\\n MNIST Weight-pruning\\n')\n",
    "for k_sparsity in k_sparsities:\n",
    "    sparse_model, score = sparsify_model(mnist_model, x_test=mnist_x_test,\n",
    "                                         y_test=mnist_y_test,\n",
    "                                         k_sparsity=k_sparsity, \n",
    "                                         pruning=pruning)\n",
    "    mnist_model_loss_weight.append(score[0])\n",
    "    mnist_model_accs_weight.append(score[1])\n",
    "    \n",
    "    # Save entire model to an H5 file\n",
    "    sparse_model.save('models/sparse_{}-model_k-{}_{}-pruned.h5'.format(dataset, k_sparsity, pruning))\n",
    "    del sparse_model\n",
    "\n",
    "\n",
    "pruning='unit'\n",
    "print('\\n MNIST Unit-pruning\\n')\n",
    "for k_sparsity in k_sparsities:\n",
    "    sparse_model, score = sparsify_model(mnist_model, x_test=mnist_x_test,\n",
    "                                         y_test=mnist_y_test, \n",
    "                                         k_sparsity=k_sparsity, \n",
    "                                         pruning=pruning)\n",
    "    mnist_model_loss_unit.append(score[0])\n",
    "    mnist_model_accs_unit.append(score[1])\n",
    "    \n",
    "    # Save entire model to an H5 file\n",
    "    sparse_model.save('models/sparse_{}-model_k-{}_{}-pruned.h5'.format(dataset, k_sparsity, pruning))\n",
    "    del sparse_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La source du code: \n",
    "https://colab.research.google.com/drive/102oKvefYhr-jrkJqR8d0wLGJmD9gNhpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Model_pruning_exploration.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
